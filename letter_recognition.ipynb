{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network to recognize handwritten letters\n",
    "***Required Packages:*** numpy, pandas, opencv-python, tensorflow, keras, scikit-learn, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "Dataset available [here](https://www.kaggle.com/datasets/sachinpatel21/az-handwritten-alphabets-in-csv-format)\n",
    "\n",
    "This dataset is comprised of images of letters that have been broken down into rows and columns of pixels. Below, we read in the dataset using pandas, and print the first 10 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"archive/A_Z_Handwritten_Data.csv\").astype('float32') # Renamed file - _ instead of spaces\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to separate our dataset into the labels and the images. In our dataset the first column contains the image labels. We can first drop the column containing the labels to assign the broken down images to the x variable, then we assign the labels to the y variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('0', axis = 1)\n",
    "y = data['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training and testing a neural network, it's useful to split our dataset into a \"Training\" and a \"Testing\" portion. The Training portion is what we'll use to train our models to detect letters, and the Testing portion is what we'll use to assess how accurate our model is. Below, we use the `train_test_split()` function to split our dataset into these two categories. After we do this, we need to reshape the data in our image variables so that they can be displayed as images. In the dataset, the images are stored as 784 columns of pixel data. Using the `np.reshape()` function, we can convert these to 28x28 matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2)\n",
    "\n",
    "train_x = np.reshape(train_x.values, (train_x.shape[0], 28,28))\n",
    "test_x = np.reshape(test_x.values, (test_x.shape[0], 28,28))\n",
    "\n",
    "print(\"Train data shape: \", train_x.shape)\n",
    "print(\"Test data shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, the labels (the letters of the alphabet) are stored using numbers from 0-25 with 0 being A and 25 being Z. Below, we create a **dictionary** where the keys correspond to the labels in number form and the values correspond to their letter counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X', 24:'Y',25:'Z'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be useful later on to know some basic characteristics of our dataset, like how frequently each letter of the alphabet is present in the training data. We can visualize this by counting how many times each label appears in the dataset and plotting these values like below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_int = np.int0(y)\n",
    "count = np.zeros(26, dtype='int')\n",
    "for i in y_int:\n",
    "    count[i] +=1\n",
    "alphabets = []\n",
    "for i in word_dict.values():\n",
    "    alphabets.append(i)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "ax.barh(alphabets, count)\n",
    "\n",
    "plt.xlabel(\"Number of elements \")\n",
    "plt.ylabel(\"Alphabets\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, it seems like some letters of the alphabet are appear more frequently in our dataset than others. This makes sens, as letters that appear less frequently like F are more distinct, and therefore more easy for our model to identify. Other letters, like O, are similar to the shapes of other letters and can be represented in many ways, therefore it might be useful to train our model more heavily on letters such as these.\n",
    "\n",
    "Now that we have an intuitive understanding of our dataset, let's plot some of the entries in the dataset to see what the images of our letters look like. To do this, we first shuffle our training dataset. Then we create a 3x3 grid of plots using the `plt.subplots()` function. Next, we need to perform what's called \"thresholding\" on our images using the `cv2.threshold()` function. This is done to try to eliminate noise from our data. After our data has been thresholded, we can plot it to get some examples of our training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuff = shuffle(train_x[:100])\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize = (10,10))\n",
    "axes = ax.flatten()\n",
    "\n",
    "for i in range(9):\n",
    "    _, shu = cv2.threshold(shuff[i], 30, 200, cv2.THRESH_BINARY)\n",
    "    axes[i].imshow(np.reshape(shuff[i], (28,28)), cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train our model, we need to do some restructing of the data so that it can be more easily interpreted by our neural network. For example, our Y variable is a label that indicates which letter is being displayed. For the purposes of our neural network, this is best represented by categorical variables rather than integers. We can convert between the two datatypes using the `to_categorical()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_x.reshape(train_x.shape[0],train_x.shape[1],train_x.shape[2],1)\n",
    "print(\"New shape of train data: \", train_X.shape)\n",
    "test_X = test_x.reshape(test_x.shape[0], test_x.shape[1], test_x.shape[2],1)\n",
    "print(\"New shape of test data: \", test_X.shape)\n",
    "train_Y = to_categorical(train_y, num_classes = 26)\n",
    "print(\"New shape of train labels: \", train_Y.shape)\n",
    "test_Y = to_categorical(test_y, num_classes = 26)\n",
    "print(\"New shape of test labels: \", test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to set up our neural network! Here, we'll be using a Convolutional Neural Network (CNN) to extract various features of the images and assign them classifying labels. As a part of our CNN, we'll add various convolutional layers that break down our images in different ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "model.add(Dense(26,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've added all of the layers in our model, it's time to actually train and validate it! To do this, we first compile our model and assign optimization and loss functions that will be used to improve fit. Finally, we initialize the training of our model using the `model.fit()` function. Since our dataset is fairly large, we'll train our model using a single \"epoch\" (or number of iterations over our training data). We also specify that the test portions of our dataset that we specified earlier will be used to check the accuracy of the model. If we find that our trained model isn't terribly accurate, we could attempt to increase the number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_X, train_Y, epochs=1,  validation_data = (test_X,test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we might be interested in knowing how accurate it is. We can output information about that using the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The validation accuracy is :\", history.history['val_accuracy'])\n",
    "print(\"The training accuracy is :\", history.history['accuracy'])\n",
    "print(\"The validation loss is :\", history.history['val_loss'])\n",
    "print(\"The training loss is :\", history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can directly test our models ability to classify letters using samples from our test dataset! To do this, we first select images from the test dataset and plot them. Then, we feed these selections into our model and ask it to predict the label of that image using the `model.predict()` function. After we've done that, we can add the prediction of each image to each plot's title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,3, figsize=(8,9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "test_X = shuffle(test_X)\n",
    "\n",
    "for i,ax in enumerate(axes):\n",
    "    img = np.reshape(test_X[i], (28,28))\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    img_reshape = np.reshape(img, (1,28,28,1))\n",
    "    pred = word_dict[np.argmax(model.predict(img_reshape))]\n",
    "    ax.set_title(\"Prediction: \"+pred)\n",
    "    ax.grid()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use our model to predict the label of images **not** contained in our testing or training datasets. Testing our model against real world data is a great way to see if it is generalizable, or if it's only successful in labeling the training data. To do this, feel free to download an image of any letter and replace the path in the code below with the images path on your computer.\n",
    "\n",
    "After our image is read in, it has to go through some processing steps in order to arrive at a format that can be interpreted by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(r'k.png')\n",
    "img_copy = img.copy()\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (400,440))\n",
    "\n",
    "img_copy = cv2.GaussianBlur(img_copy, (7,7), 0)\n",
    "img_gray = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n",
    "_, img_thresh = cv2.threshold(img_gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "img_final = cv2.resize(img_thresh, (28,28))\n",
    "img_final =np.reshape(img_final, (1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred = word_dict[np.argmax(model.predict(img_final))]\n",
    "cv2.putText(img, \"Prediction: \" + img_pred, (20,410), cv2.FONT_HERSHEY_DUPLEX, 1.3, color = (255,0,30))\n",
    "cv2.imshow(\"Letter Recognition\", img)\n",
    "\n",
    "while (1):\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == 27:                 # Press escape to close window\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
